{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Relazione Multimedia (LM-18)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduzione** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'obiettivo di questo progetto è restaurare video d'epoca, con particolare attenzione all'incremento della risoluzione tramite algoritmi di machine learning come SwinIR e Real-ESRGAN, oltre al miglioramento dei bordi e alla riduzione del rumore.\n",
    "\n",
    "\n",
    "Ecco un esempio di **introduzione** che puoi usare per il tuo progetto:\n",
    "\n",
    "---\n",
    "\n",
    "**Introduzione**\n",
    "\n",
    "Il restauro dei video d'epoca rappresenta una sfida affascinante nell'ambito del trattamento delle immagini e del video, poiché coinvolge la preservazione e l'ottimizzazione di contenuti storici che potrebbero essere danneggiati o di bassa qualità. Con l'avvento delle tecniche di **machine learning** e dell'**intelligenza artificiale**, è oggi possibile applicare algoritmi avanzati per migliorare la qualità visiva e aumentare la risoluzione di video storici, rendendoli più fruibili e dettagliati.\n",
    "\n",
    "L'obiettivo principale di questo progetto è il restauro dei video d'epoca, con un focus particolare sull'incremento della risoluzione e sul miglioramento della qualità complessiva del video. Per questo scopo, verranno utilizzati due algoritmi di **super-risoluzione** basati su **machine learning**: **SwinIR** e **Real-ESRGAN**. Questi algoritmi sono in grado di ricostruire dettagli ad alta risoluzione da immagini o video a bassa risoluzione, applicando avanzati metodi di **deep learning**.\n",
    "\n",
    "Oltre al miglioramento della risoluzione, il progetto mira a migliorare i **bordo** dei video restaurati e a ridurre il **rumore** visivo, un aspetto fondamentale nei video d'epoca che spesso sono soggetti a distorsioni. Grazie all'uso di tecniche avanzate come l'**elaborazione dei bordi** e la **riduzione del rumore**, si prevede di ottenere un risultato finale che non solo conserva il valore storico del video, ma lo rende anche visivamente più nitido e godibile per un pubblico moderno.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Metodi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SwinIR**\n",
    "\n",
    "link: https://arxiv.org/pdf/2108.10257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SwinIR è composto da tre parti: **shallow feature extraction**, **deep feature extraction** e **high-quality image reconstruction**. In particolare, il modulo deep feature extraction è costituito da diversi blocchi di Swin Transformer blocks (RSTB), ognuno dei quali include diversi layer Swin Transformer layers insieme a una connessione residua. \n",
    "\n",
    "Gli esperimenti su diverse attività, come la super-risoluzione, il denoising e la riduzione degli artefatti JPEG, hanno dimostrato che SwinIR supera i metodi esistenti, migliorando la qualità dell'immagine e riducendo i parametri del modello fino al 67%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SwinIR fa uso di **Transformer**, noto nel campo dell'elaborazione del linguaggio naturale, è stato adottato nella computer vision community per diversi compiti, come classificazione, rilevamento oggetti, segmentazione e conteggio delle folle, grazie alla sua capacità di esplorare le interazioni globali tra le regioni di un'immagine. È stato anche introdotto nel restauro delle immagini, ma modelli come IPT e VSR-Transformer presentano sfide legate all'elevato numero di parametri e alla necessità di grandi dataset. Inoltre, entrambi usano l'attenzione a livello di patch, che potrebbe non essere ideale per il restauro delle immagini. Un approccio parallelo ha esplorato l'uso del Swin Transformer in un'architettura a forma di U per affrontare queste problematiche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"img_relazione/swinIR_net.png\" alt=\"SwinIt network\" width=\"60%\" height=\"60%\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'immagine illustra la struttura di SwinIR, che si compone di tre moduli principali: estrazione delle caratteristiche superficiali, estrazione delle caratteristiche profonde e ricostruzione dell'immagine ad alta qualità (HQ). SwinIR consente di selezionare il tipo di operazione desiderata tra denoising, restauro ad alta risoluzione e riduzione degli artefatti da compressione JPEG. Pur utilizzando gli stessi moduli di estrazione delle caratteristiche per tutti i compiti di restauro, vengono impiegati moduli di ricostruzione differenti a seconda del compito specifico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Shallow and deep feature extraction**\n",
    "Il processo di estrazione delle caratteristiche in SwinIR avviene in due fasi: superficiale e profonda. Nella prima fase, viene utilizzato uno strato convoluzionale 3x3 per estrarre le caratteristiche superficiali da un'immagine di bassa qualità. Nella fase successiva, le caratteristiche superficiali vengono ulteriormente elaborate attraverso un modulo di estrazione delle caratteristiche profonde, che include più blocchi residui Swin Transformer (RSTB) e un altro strato convoluzionale. Questo approccio combina i benefici delle convoluzioni iniziali con l'auto-attenzione dei Transformer, creando una solida base per l'aggregazione delle caratteristiche superficiali e profonde per il restauro dell'immagine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **HQ Image reconstruction**\n",
    "In SwinIR, per la super-risoluzione dell'immagine (SR), l'immagine ad alta qualità viene ricostruita aggregando le caratteristiche superficiali e profonde. Le caratteristiche superficiali contengono basse frequenze, mentre quelle profonde si concentrano sul recupero delle alte frequenze. Una lunga connessione saltata consente di trasmettere direttamente l'informazione a bassa frequenza al modulo di ricostruzione, migliorando la stabilità dell'addestramento. \n",
    "\n",
    "Il modulo di ricostruzione utilizza una convoluzione sub-pixel per aumentare la risoluzione. Per compiti come il denoising e la riduzione degli artefatti JPEG, viene usato un singolo strato convoluzionale. Inoltre, SwinIR utilizza l'apprendimento residuo per ricostruire la differenza tra l'immagine di bassa qualità (LQ) e quella ad alta qualità (HQ), migliorando l'efficienza del restauro.\n",
    "\n",
    "**Apprendimento residuo** : In una rete neurale tradizionale, la rete cerca di apprendere una funzione diretta che mappa un input x a un output y. Tuttavia, nei modelli basati su residual learning, la rete cerca di imparare una differenza o residuo tra l'output desiderato e l'output effettivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **Residual Swin Transformer Block**\n",
    " **(RSTB)** è un blocco residuo che combina strati di **Swin Transformer (STL)** e strati convoluzionali. Inizia estraendo caratteristiche intermedie tramite più strati Swin Transformer, quindi aggiunge uno strato convoluzionale prima della connessione residua. L'output finale viene ottenuto sommando l'output del convoluzionale con l'input originale del blocco. La connessione residua permette l'aggregazione di caratteristiche a vari livelli, favorendo una connessione diretta tra i blocchi e il modulo di ricostruzione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Swin Transformer layer**\n",
    "**(STL)** si basa sull'auto-attention multi-head, ma introduce due principali innovazioni: l'**attenzione locale** e il meccanismo delle \n",
    "**finestre spostate**.\n",
    "\n",
    "L'**auto-attention multi-head** è un meccanismo centrale nei modelli Transformer, utilizzato per elaborare e comprendere le relazioni tra diverse parti di una sequenza di input (ad esempio, parole in una frase o pixel in un'immagine). Questo meccanismo consente al modello di \"prestare attenzione\" a diverse parti dell'input simultaneamente, migliorando la capacità del modello di catturare dipendenze complesse tra le informazioni.\n",
    "\n",
    "1. **Preparazione dell'input**: L'input viene diviso in finestre locali non sovrapposte di dimensione \\( M x M \\), e per ciascuna finestra viene calcolata l'auto-attention locale.\n",
    "\n",
    "2. **Auto-attention locale**: Le matrici di query \\( Q \\), chiave \\( K \\), e valore \\( V \\) sono calcolate per ciascuna finestra, e l'attention è calcolata separatamente per ogni finestra tramite una SoftMax normalizzata.\n",
    "\n",
    "3. **Multi-Head Self-Attention (MSA)**: L'auto-attention viene eseguita \\( h \\) volte in parallelo e i risultati vengono concatenati.\n",
    "\n",
    "4. **Trasformazione delle caratteristiche**: Una rete MLP con due strati completamente connessi e GELU è utilizzata per ulteriori trasformazioni delle caratteristiche.\n",
    "\n",
    "5. **Finestre spostate**: Per abilitare connessioni tra finestre, vengono alternate partizioni regolari e partizioni con finestre spostate, migliorando l'interazione tra diverse finestre locali.\n",
    "\n",
    "Questo design consente al modello di catturare sia le informazioni locali che le relazioni a lungo raggio nelle immagini.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Real-ESRGAN**\n",
    "\n",
    "link: https://arxiv.org/pdf/2107.10833"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-ESRGAN** è un modello di super-risoluzione delle immagini che viene addestrato esclusivamente con dati sintetici. Per migliorare la simulazione delle **degradazioni complesse del mondo reale**, il modello introduce un processo di **modellazione della degradazione di ordine superiore**. Inoltre, vengono considerati artefatti comuni, come il **ringing** e l'**overshoot**, che possono verificarsi durante il processo di sintesi.\n",
    "\n",
    "- **Ringing**: Artefatti visibili come onde o oscillazioni ai bordi, causati dalla perdita di alta frequenza.\n",
    "- **Overshoot**: Eccessiva saturazione o picchi nei valori di pixel, spesso visibili ai bordi degli oggetti o nelle transizioni di colore, causati da un'eccessiva amplificazione durante il processo di ricostruzione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"img_relazione/ESRGAN_de.png\" alt=\"SwinIt network\" width=\"80%\" height=\"80%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'immagine mostra i processi usati per la generazione di dati sintetici puri per simulare degradazioni pratiche in modo più realistico. Il modello adotta un processo di degradazione di secondo ordine, dove ogni degradazione segue il modello classico di degradazione. Le scelte dettagliate per i vari tipi di degradazione, come sfocatura, ridimensionamento, rumore e compressione JPEG, sono specificate nel processo.\n",
    "\n",
    "Inoltre, per sintetizzare artefatti comuni come il ringing e l'overshoot, viene impiegato un filtro sinc. Questo approccio aiuta a generare immagini sintetiche che rispecchiano meglio le complessità delle degradazioni nel mondo reale.\n",
    "\n",
    "$$\n",
    "\\text{sinc}(x) = \\frac{\\sin(\\pi x)}{\\pi x}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"img_relazione/ESRGAN_net.png\" alt=\"SwinIt network\" width=\"80%\" height=\"80%\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Real-ESRGAN** adotta lo stesso **network generatore** di **ESRGAN**. Per i fattori di scala **×2** e **×1**, inizialmente esegue un'operazione di **pixel-unshuffle** per ridurre la dimensione spaziale e riorganizzare le informazioni nella dimensione dei canali. \n",
    "Questa operazione aiuta a preparare i dati per un successivo miglioramento della risoluzione, ottimizzando il processo di super-risoluzione nelle immagini.\n",
    "\n",
    "**ESRGAN** utilizza l'architettura di base di **SRResNet**, dove la maggior parte dei calcoli avviene nello spazio delle **caratteristiche a bassa risoluzione (LR)**. È possibile selezionare o progettare **\"blocchi base\"** (ad esempio, il **residual block**, **dense block**, **RRDB**) per migliorare le prestazioni del modello. \n",
    "\n",
    "Per migliorare ulteriormente la qualità dell'immagine recuperata da **SRGAN**, vengono apportate due modifiche principali alla struttura del generatore **G**:\n",
    "\n",
    "1. **Rimozione di tutti i layer BN** (Batch Normalization).\n",
    "2. **Sostituzione del blocco base originale** con il nuovo **Residual-in-Residual Dense Block (RRDB)**, che combina una **rete residua multi-livello** e **connessioni dense**.\n",
    "\n",
    "Queste modifiche mirano a migliorare le prestazioni del modello nella **super-risoluzione** delle immagini, ottimizzando sia la qualità visiva che la stabilità del processo di training.\n",
    "\n",
    "ESRGAN generator: Adottiamo lo stesso generatore (rete SR) di ESRGAN, ovvero una rete profonda con diversi Residual-in-Residual Dense Blocks (RRDB). Estendiamo anche l'architettura originale ×4 ESRGAN per eseguire la super-risoluzione con un fattore di scala di ×2 e ×1.\n",
    "\n",
    "Poiché ESRGAN è una rete pesante, inizialmente impieghiamo l'operazione di pixel-unshuffle (un'operazione inversa del pixel-shuffle) per ridurre la dimensione spaziale ed espandere la dimensione dei canali prima di fornire gli input alla principale architettura ESRGAN. In questo modo, la maggior parte dei calcoli viene eseguita in uno spazio a risoluzione più piccola, riducendo così il consumo di memoria GPU e risorse computazionali.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mettere i valori di denoisinfg usati "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Risultati**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusione**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
